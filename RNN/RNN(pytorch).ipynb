{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "python 3.8 + pytorch 1.8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 搭建Vanilla RNN\n",
    "\n",
    "- 目的：搭建简单的RNN模型并进行模拟句子分类\n",
    "- 主要步骤：\n",
    "1. 输入尺寸分析\n",
    "2. 构建模型\n",
    "3. 模型初始化\n",
    "4 .测试"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##  1. 输入尺寸分析\n",
    "\n",
    "- 推荐阅读材料：\n",
    "[LSTM pytorch参数理解](https://zhuanlan.zhihu.com/p/340763181)-知乎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7984,  2.9845, -0.0627, -2.5412, -1.4661, -1.1345,  1.2943,\n",
      "           1.0295,  0.3614,  0.9838],\n",
      "         [-2.2107,  1.1269, -0.1324,  1.5966, -0.2545,  0.1964,  0.5977,\n",
      "           0.0629,  1.6581, -1.8710],\n",
      "         [ 0.4070,  1.2150, -0.1763, -0.6439, -0.6503,  0.0745, -0.2769,\n",
      "           0.0507,  2.1819, -1.5359],\n",
      "         [ 0.4931,  0.4234, -2.0911, -0.3107, -0.8948,  0.2314,  0.6304,\n",
      "          -1.0792,  0.1387, -2.1024],\n",
      "         [-2.7084,  1.9043,  1.0090,  2.0205,  0.6874,  0.6515, -0.5708,\n",
      "           0.2782, -2.0245,  1.5090]],\n",
      "\n",
      "        [[-0.2331,  0.8968,  0.1058, -0.8904,  0.9551, -0.3846,  1.1521,\n",
      "           0.3353, -0.0336,  1.8635],\n",
      "         [-0.5744, -0.1069,  0.4371, -1.8343,  0.4586, -0.2221, -1.0703,\n",
      "           1.4535, -0.0518,  1.4833],\n",
      "         [-0.2980,  0.7146,  0.0996, -1.2401, -1.0239,  0.8318, -0.5858,\n",
      "          -0.7593,  0.7798, -0.5904],\n",
      "         [-0.0862,  0.5282,  1.2474,  1.0868,  0.5100,  1.4611, -0.3139,\n",
      "          -1.0567,  1.4279, -0.4733],\n",
      "         [-0.9125, -0.9176, -1.7832, -0.9372,  0.9956, -0.3283,  0.7386,\n",
      "           1.0746,  1.5152, -0.3516]]])\n",
      "torch.Size([2, 5, 10])\n",
      "torch.Size([2, 5, 30])\n",
      "torch.Size([1, 5, 30])\n",
      "torch.Size([1, 5, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchstat import stat\n",
    "from torchsummary import summary\n",
    "# input_size 为输入数据维数，即单词稀疏向量长度\n",
    "# hidden_size 为输出维数，例如有30个分类\n",
    "lstm = nn.LSTM(input_size=10, hidden_size=30, num_layers=1)\n",
    "\n",
    "seq_len = 2 # 表示参与训练的序列长度（句子长度）\n",
    "batch_size = 5 # batch_size代表一批几个序列参与训练（几个句子参与训练）\n",
    "input_size = 10 # 单词稀疏向量长度\n",
    "# 模拟数据\n",
    "x = torch.randn((seq_len, batch_size, input_size))\n",
    "print(x)\n",
    "y, (h, c) = lstm(x)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(h.shape) # 单向单层LSTM隐藏状态0维为1\n",
    "print(c.shape) # 同h\n",
    "# stat(lstm, input_size=(2,10,1))\n",
    "# summary(lstm.cuda(), input_size=(2,10,), batch_size=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 构建模型："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# 结构：输入层->embed层->lstm层->fc层\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)  # 词向量层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)  # LSTM 层\n",
    "        # 全连接输出分类层, output_size 为类别大小，输出各词概率\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, batch_size]\n",
    "        embeds = self.embed(x)  # 经由词向量层\n",
    "        # embeds: [seq_len, batch_size, embedding_dim]\n",
    "        lstm_out, _ = self.lstm(embeds)  # 经同由 LSTM 层\n",
    "        # lstm_out: [seq_len, batch_size, hidden_dim]\n",
    "        y = self.fc(lstm_out[-1])  # 取最后一步的输出进入最后的分类层\n",
    "        # y: [batch_size, output_size]\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 定义全局变量\n",
    "EMBEDDING_DIM = 128  # 词向量的大小为 128，embedding层输出维度\n",
    "HIDDEN_DIM = 216  # 隐层大小为 216,LSTM层输出维度\n",
    "VOCAB_DIM = 1000  # 词典大小为 1000，embedding_lookup词典大小\n",
    "OUTPUT_SIZE = 3  # 输出类别为 3 类\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 模型初始化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_1648/4023137565.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mmy_lstm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mLSTM\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEMBEDDING_DIM\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mHIDDEN_DIM\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mVOCAB_DIM\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mOUTPUT_SIZE\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmy_lstm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0msummary\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmy_lstm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32md:\\programdata\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torchsummary\\torchsummary.py\u001B[0m in \u001B[0;36msummary\u001B[1;34m(model, input_size, batch_size, device)\u001B[0m\n\u001B[0;32m     70\u001B[0m     \u001B[1;31m# make a forward pass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m     \u001B[1;31m# print(x.shape)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m     \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     73\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m     \u001B[1;31m# remove these hooks\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\programdata\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_1648/3279174175.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m         \u001B[1;31m# x: [seq_len, batch_size]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m         \u001B[0membeds\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# 经由词向量层\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m         \u001B[1;31m# embeds: [seq_len, batch_size, embedding_dim]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m         \u001B[0mlstm_out\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlstm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membeds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# 经同由 LSTM 层\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\programdata\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\programdata\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    156\u001B[0m         return F.embedding(\n\u001B[0;32m    157\u001B[0m             \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding_idx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_norm\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 158\u001B[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001B[0m\u001B[0;32m    159\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    160\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\programdata\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   1914\u001B[0m         \u001B[1;31m# remove once script supports set_grad_enabled\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1915\u001B[0m         \u001B[0m_no_grad_embedding_renorm_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_norm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnorm_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1916\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding_idx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscale_grad_by_freq\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msparse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1917\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1918\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "my_lstm = LSTM(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_DIM, OUTPUT_SIZE)\n",
    "my_lstm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 测试"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "tensor([[ 0.0347,  0.0891, -0.0613],\n",
      "        [-0.0052,  0.0740, -0.0148]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([2, 3])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seq_len 为 2，batch_size 为 4，各数字表示某单词的 id\n",
    "x = torch.tensor([[1, 4, 5, 5], [3, 4, 9, 5]])\n",
    "print(x.permute(1, 0).shape)\n",
    "# x 大小为 [batch_size, seq_len],需要转置\n",
    "y = my_lstm(x.permute(1, 0))# 每个batch输出一个分类概率向量\n",
    "print(y) # y 大小为[seq_len, 3]\n",
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}