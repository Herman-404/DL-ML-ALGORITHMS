{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "python 3.8 + pytorch 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 目的：搭建seq2seq+attention模型并进行模拟语句预测\n",
    "- 输入：id序列(input)\n",
    "- 输出：预测id序列(decoder_output)\n",
    "- 主要步骤：\n",
    " 1. 构建编码器\n",
    " 2. 构建译码器，并加入attention机制\n",
    " 3. 编码器译码器模型初始化\n",
    " 4. 训练及测试"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 构建基于GRU的编码器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size  # 隐层大小\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)  # 词向量层\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)  # GRU 层\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # 输入的 batch_size 为 1，且每次输入一个单词\n",
    "        embedded = self.embedding(input).view(1, 1, -1)  # 词向量计算，注意输出后的维度变化\n",
    "        output, hidden = self.gru(embedded, hidden)  # GRU 层计算\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):  # 初始化隐状态\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 构建基于 GRU 的解码器，并且加入 attention 机制"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# forward过程：\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size  # 隐层大小\n",
    "        self.output_size = output_size  # 输出层大小，实际上为目标语言的词典大小\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.output_size, self.hidden_size)  # 词向量层\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)  # GRU 层\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)  # 输出层\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)  # 输入当前单词的词向量计算\n",
    "        embedded = F.relu(embedded)  # 激活函数 Relu\n",
    "        output, hidden = self.gru(embedded, hidden)  # GRU 运算\n",
    "\n",
    "        # 注意力运算，由 encoder_outputs 和 hidden 进行内积计算\n",
    "        attn_weights = F.softmax(torch.bmm(encoder_outputs.unsqueeze(\n",
    "            0), hidden.view(1, self.hidden_size, -1)), dim=1)\n",
    "        # 添加注意力机制的源文信息，获得加权和后的向量表征\n",
    "        weighted_context = torch.matmul(\n",
    "            attn_weights.squeeze(2), encoder_outputs)\n",
    "        # 拼接当前信息以及添加注意力机制的源文信息\n",
    "        output = torch.cat([output.squeeze(0), weighted_context], dim=1)\n",
    "        # 对输出进行 softmax 计算\n",
    "        output = F.softmax(self.out(output), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):  # 初始化隐状态\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 初始化编码器及解码器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "INPUT_SIZE = 1000  # 输入维度，实际上为源语言的词典大小\n",
    "HIDDEN_SIZE = 60  # 隐层大小\n",
    "OUTPUT_SIZE = 1000  # 输出维度，实际上为目标语言的词典大小\n",
    "\n",
    "# 初始化编码器及解码器\n",
    "my_encoder = EncoderRNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE)\n",
    "my_decoder = AttnDecoderRNN(hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE)\n",
    "my_encoder,my_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 训练及测试"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 输入假设的源语句，进行编码过程\n",
    "import torch\n",
    "\n",
    "# 假设已转换为 id 的输入源语句如下\n",
    "input = torch.tensor([[0], [4], [3], [1]])\n",
    "# 初始化 encoder 隐状态\n",
    "encoder_hidden = my_encoder.initHidden()\n",
    "# 获取输入文本长度\n",
    "input_length = input.size(0)\n",
    "# 用于存在 encoder 每一步的输出\n",
    "encoder_outputs = torch.zeros(input_length, my_encoder.hidden_size)\n",
    "# 每一步进行 rnn(GRU) 运算，并将结果保存，得到 encoder_outputs\n",
    "for i in range(input_length):\n",
    "    encoder_output, encoder_hidden = my_encoder(input[i], encoder_hidden)\n",
    "    encoder_outputs[i] = encoder_output[0, 0]\n",
    "\n",
    "encoder_outputs.shape # shape = [input_length, hidden_size]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1000])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder 首输入为 \"<s>\"，假设其对应 id 为 0\n",
    "decoder_input = torch.tensor([[0]])\n",
    "# encoder 隐状态传给 decoder，作为其初始隐状态\n",
    "decoder_hidden = encoder_hidden\n",
    "# 进行一步编码过程\n",
    "decoder_output, decoder_hidden, decoder_attention = my_decoder(\n",
    "    decoder_input, decoder_hidden, encoder_outputs)\n",
    "decoder_output.shape  # [1,output_size], 在输出结果中选概率最高者作为编码出的单词\n",
    "# TODO：重复上一步进行编码\n",
    "# 最终编码输出是在 output_size 维中选取概率最大的单词作为所编码出的词汇，\n",
    "# 接着将其作为下一步输入，再进入下一步的编码过程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}